{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これからspaCy（日本語版）の練習をしていく。\n",
    "「spaCyを使った先進的な自然言語処理」というサイトに載っている問題集を中心に進めていく。\n",
    "https://course.spacy.io/ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Japanese version, I had to disable LUA(Limited User Account). According to Microfost, if it is set as false (0), \"Windows does not notify the user when programs try to install software or make changes to the computer.\n",
    "We do not recommend using this setting, but it can be selected for systems that use programs \n",
    "that are not certified for Windows 8, Windows Server 2012, Windows 7 or Windows Server 2008 R2 \n",
    "because they do not support UAC(User Account Controls).\"\n",
    "\n",
    "To disable LUA, here is what I did: \n",
    "Windows logo key + R, type regedit\n",
    "H key local machine -> software -> Microsoft -> Windows -> Current version -> Policies -> System -> Enable LUA -> 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ja_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp('今日からSpaCyの日本語バージョンを試してみます。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 NOUN obl\n",
      "から ADP case\n",
      "SpaCy NOUN nmod\n",
      "の ADP case\n",
      "日本 PROPN compound\n",
      "語 NOUN compound\n",
      "バージョン NOUN obj\n",
      "を ADP case\n",
      "試し VERB ROOT\n",
      "て SCONJ mark\n",
      "み AUX aux\n",
      "ます AUX aux\n",
      "。 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for w in text:\n",
    "    print(w.text, w.pos_, w.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:今日, pos:NOUN, tag:名詞-普通名詞-副詞可能, lemma:今日\n",
      "text:から, pos:ADP, tag:助詞-格助詞, lemma:から\n",
      "text:SpaCy, pos:NOUN, tag:名詞-普通名詞-一般, lemma:spacy\n",
      "text:の, pos:ADP, tag:助詞-格助詞, lemma:の\n",
      "text:日本, pos:PROPN, tag:名詞-固有名詞-地名-国, lemma:日本\n",
      "text:語, pos:NOUN, tag:名詞-普通名詞-一般, lemma:語\n",
      "text:バージョン, pos:NOUN, tag:名詞-普通名詞-一般, lemma:バージョン\n",
      "text:を, pos:ADP, tag:助詞-格助詞, lemma:を\n",
      "text:試し, pos:VERB, tag:動詞-一般, lemma:試す\n",
      "text:て, pos:SCONJ, tag:助詞-接続助詞, lemma:て\n",
      "text:み, pos:AUX, tag:動詞-非自立可能, lemma:みる\n",
      "text:ます, pos:AUX, tag:助動詞, lemma:ます\n",
      "text:。, pos:PUNCT, tag:補助記号-句点, lemma:。\n"
     ]
    }
   ],
   "source": [
    "for w in text:\n",
    "    print(f'text:{w.text}, pos:{w.pos_}, tag:{w.tag_}, lemma:{w.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(textfile):\n",
    "    for token in textfile:\n",
    "        print(f'{token.text:{15}} {token.pos_:{15}} {token.tag_:{20}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日              NOUN            名詞-普通名詞-副詞可能         今日\n",
      "から              ADP             助詞-格助詞               から\n",
      "SpaCy           NOUN            名詞-普通名詞-一般           spacy\n",
      "の               ADP             助詞-格助詞               の\n",
      "日本              PROPN           名詞-固有名詞-地名-国         日本\n",
      "語               NOUN            名詞-普通名詞-一般           語\n",
      "バージョン           NOUN            名詞-普通名詞-一般           バージョン\n",
      "を               ADP             助詞-格助詞               を\n",
      "試し              VERB            動詞-一般                試す\n",
      "て               SCONJ           助詞-接続助詞              て\n",
      "み               AUX             動詞-非自立可能             みる\n",
      "ます              AUX             助動詞                  ます\n",
      "。               PUNCT           補助記号-句点              。\n"
     ]
    }
   ],
   "source": [
    "show_lemmas(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('テキスト処理を学びます。最初のトークンを選んでみます。')\n",
    "token1 = doc2[0]\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキスト NOUN compound\n",
      "処理 NOUN obj\n",
      "を ADP case\n",
      "学び VERB ROOT\n",
      "ます AUX aux\n",
      "。 PUNCT punct\n",
      "最初 NOUN nmod\n",
      "の ADP case\n",
      "トークン NOUN obj\n",
      "を ADP case\n",
      "選ん VERB ROOT\n",
      "で SCONJ mark\n",
      "み AUX aux\n",
      "ます AUX aux\n",
      "。 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最初のトークン\n"
     ]
    }
   ],
   "source": [
    "# 「最初のトークン」のみを表示\n",
    "# 最後の数字にあたるトークンは表示されないので注意\n",
    "\n",
    "first_token = doc2[6:9]\n",
    "print(first_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# spaCyのDocとTokenオブジェクトと、その語彙属性（lexical attributes）を使って、 文字列の中からパーセンテージを表す部分を抜き出す。\n",
    "# つまり、数字とパーセント記号からなる連続した二つのトークンを探す。\n",
    "# 例文＆コードは以下のウェブサイトから取得：\n",
    "# https://course.spacy.io/ja/chapter1\n",
    "\n",
    "doc2 = nlp(\n",
    "    \"1990年には東アジアの60%以上の人々が極度の貧困状態に陥っていました。\"\n",
    "    \"今では4%以下になっています。\")\n",
    "\n",
    "for token in doc2:\n",
    "    # トークンが数字に似ているかどうかをチェック\n",
    "    if token.like_num:\n",
    "        # docの次のトークンを取得\n",
    "        next_token = doc2[token.i + 1]\n",
    "        # 次のトークンの文字列が「%」に一致するかどうかをチェック\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 70.4\n",
      "Percentage found: 83.7\n",
      "Percentage found: 84.3\n",
      "Percentage found: 84.7\n"
     ]
    }
   ],
   "source": [
    "# 自分でコードを試してみる。\n",
    "# 参考URL：\n",
    "# https://news.yahoo.co.jp/articles/50ecf18eeecfe9f383f67b84f424d651e27edd37\n",
    "\n",
    "doc3 = nlp('最も売れなかったのは「かぜ薬」(金額前年比70.4％:以下同じ)だ。'\n",
    "          'その他、「リップクリーム」(83.7％)、「パック」(84.3％)、「ファンデーション」(84.7％)などは、在宅勤務の普及や外出を控える行動様式をストレートに反映した結果といえる。'\n",
    "          )\n",
    "\n",
    "for token in doc3: \n",
    "    if token.like_num:\n",
    "        next_token = doc3[token.i +1]\n",
    "        if next_token.text == \"％\":    # イコールは必ず２つ。next_tokenのあとの.textを忘れない\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：％が全角（原文通り）だとPercentage foundが返されるが、半角の%では何も返されない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
